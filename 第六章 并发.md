并发及其表现形式之一，即并行处理，是软件工程领域最广泛的主题之一。并发是一个巨大的话题，可以写成几十本书，但我们仍然无法讨论它的所有重要方面和模型。本章的目的是向你展示为什么在你的应用程序中可能需要并发、何时使用它以及 Python 最重要的并发模型是什么。

我们将讨论一些允许你在代码中实现这些模型的语言特性、内置模块和第三方包。但我们不会详细介绍它们。将本章内容视为你自己研究和阅读的切入点。我们将尝试引导你了解基本思想，并帮助你决定是否真的需要并发。希望在阅读本章后，你将能够判断哪种方法最适合你的需求。

在本章中，我们将涵盖以下主题：

- 什么是并发？
- 多线程
- 多进程
- 异步编程

在我们进入并发的基本概念之前，让我们首先考虑技术要求。

## 技术要求

以下是本章用到的Python包，可以从PyPI下载：

- requests
- aiohttp

关于如何安装包的信息包含在第 2 章，现代 Python 开发环境中。

> 本章的代码文件可以在 https://github.com/PacktPublishing/Expert-Python-Programming-Fourth-Edition/tree/main/Chapter%206 找到。

在我们深入研究 Python 程序员可用的各种并发实现之前，让我们讨论一下并发实际上是什么。

## 什么是并发？

并发经常与实现它的实际方法相混淆。一些程序员还认为它是并行处理的同义词。这就是为什么我们需要从正确定义并发开始的原因。只有这样，我们才能正确理解各种并发模型及其主要区别。

首先，并发性与并行性不同。并发性也不是应用程序实现的问题。并发性是程序、算法或问题的属性，而并行性只是解决并发问题的一种可能方法。

在 Leslie Lamport 1976 年的论文时间、时钟和分布式系统中的事件排序中，他将并发的概念定义如下：

> “如果两个事件都不能对另一个产生因果影响，那么这两个事件就是并发的。”

通过将事件外推到程序、算法或问题，如果某事物可以完全或部分分解为与顺序无关的组件（单元），我们就可以说它是并发的。这些单元可以相互独立处理，处理顺序不影响最终结果。这意味着它们也可以同时或并行处理。如果我们以这种方式（即并行）处理信息，那么我们确实在处理并行处理。但这仍然不是强制性的。

以分布式方式工作，最好使用多核处理器或计算集群的能力，是并发问题的自然结果。无论如何，这并不意味着这是有效处理并发的唯一方法。有很多用例可以通过同步方式以外的方式解决并发问题，但不需要并行执行。换句话说，当一个问题同时发生时，它让你有机会以一种特殊的、最好是更有效的方式来处理它。

我们经常习惯于以经典的方式解决问题：通过执行一系列步骤。这就是我们大多数人思考和处理信息的方式——使用同步算法，一次只做一件事，一步一步。但是这种处理信息的方式不太适合解决大规模问题或需要同时满足多个用户或软件代理的需求时：

- 当处理作业的时间受限于单个处理单元（单机、CPU 核等）的性能时
- 当你在程序完成前一个输入之前无法接受和处理新输入时

这些问题产生了三种常见的应用场景，其中并发处理是满足用户需求的可行方法：

- 处理分布：问题的规模如此之大，以至于在可接受的时间范围内（资源有限）处理它的唯一方法是在可以并行处理工作的多个处理单元上分配执行。
- 应用程序响应性：你的应用程序需要保持响应性（接受新输入），即使它没有完成处理先前的输入。
- 后台处理：并非每个任务都需要以同步方式执行。如果不需要立即访问特定操作的结果，及时推迟执行可能是合理的。

处理分布场景直接映射到并行处理。这就是为什么它通常用多线程和多进程模型来解决。应用程序响应场景通常不需要并行处理，因此实际解决方案实际上取决于问题细节。应用程序响应性问题还涵盖应用程序需要独立为多个客户端（用户或软件代理）提供服务的情况，而无需等待其他客户端成功服务。

有趣的是，这些问题组并不是排他性的。通常，你必须保持应用程序的响应能力，同时无法在单个处理单元上处理所有输入。这就是为什么经常同时使用不同的、看似替代或冲突的并发方法的原因。这在 Web 服务器的开发中尤为常见，在这种情况下，可能需要使用异步事件循环或线程与多个进程结合使用，以便利用所有可用资源并在高负载下仍然保持低延迟。

Python 提供了几种处理并发的方法。这些主要是：

- 多线程：其特点是运行多个共享父进程内存上下文的执行线程。它是最流行（也是最古老）的并发模型之一，在执行大量 I/O（输入/输出）操作或需要保持用户界面响应能力的应用程序中效果最好。它相当轻巧，但有很多注意事项和内存安全风险。
- 多进程：其特点是运行多个独立的进程，以分布式方式执行工作。它类似于运行中的线程，尽管它不依赖于共享内存上下文。由于 Python 的特性，它更适合 CPU 密集型应用程序。它比多线程更重要，并且需要实现进程间通信模式来协调进程之间的工作。
- 异步编程：其特点是在单个应用程序进程中运行多个协作任务。协作任务像线程一样工作，尽管它们之间的切换是由应用程序本身而不是操作系统内核促进的。它非常适合 I/O 密集型应用程序，尤其是需要处理多个并发网络连接的程序。异步编程的缺点是需要使用专用的异步库。

我们将详细讨论的第一个模型是多线程。

## 多线程

开发人员通常认为多线程是一个非常复杂的话题。虽然这种说法完全正确，但 Python 提供了对使用线程有很大帮助的高级类和函数。 CPython 有一些不方便的实现细节，这使得线程不如 C 或 Java 等其他编程语言有效。但这并不意味着它们在 Python 中完全没用。

仍然有相当多的问题可以用 Python 线程有效和方便地解决。

在本节中，我们将讨论 CPython 中多线程的那些限制，以及 Python 线程仍然是可行解决方案的常见并发问题。

### 什么是多线程？

线程是执行线程的缩写。程序员可以将他们的工作拆分为同时运行的线程。线程仍然绑定到父进程并且可以轻松通信，因为它们共享相同的内存上下文。线程的执行由操作系统内核协调。

多线程将受益于多进程器或多核机器，其中每个线程都可以在单独的 CPU 内核上执行，从而使程序运行得更快。这是适用于大多数编程语言的一般规则。在 Python 中，多核 CPU 上的多线程带来的性能优势有一些限制，我们将在后面讨论。为简单起见，让我们暂时假设该语句也适用于 Python。

使用 Python 启动新执行线程的最简单方法是使用 threading.Thread() 类，如下例所示：

```python
def my_function():
    print("printing from thread")
if __name__ == "__main__":
    thread = Thread(target=my_function)
    thread.start()
    thread.join()
```

my_function() 函数就是我们要在新线程中执行的函数。我们将它作为目标关键字参数传递给 Thread 类构造函数。此类的实例用于封装和控制应用程序线程。

创建一个新的 Thread 类实例不足以启动一个新线程。为此，你需要调用 start() 方法。一旦新线程启动，它将在主线程旁边运行，直到目标函数完成。在上面的例子中，我们使用 join() 方法显式地等待额外的线程完成。

> 我们说 join() 方法是一个阻塞操作。这意味着线程没有做任何特别的事情（它不消耗 CPU 时间），只是等待特定事件的发生。

start() 和 join() 方法允许你一次创建和启动多个线程。下面是对上一个示例的简单修改，批量启动和加入多个线程：

```python
from threading import Thread
def my_function():
    print("printing from thread")
if __name__ == "__main__":
    threads = [Thread(target=my_function) for _ in range(10)]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()
```

所有线程共享相同的内存上下文。这意味着你必须非常小心你的线程如何访问相同的数据结构。如果两个并行线程在没有任何保护的情况下更新同一个变量，则可能会出现线程执行中细微的时间变化可能以意想不到的方式改变最终结果的情况。为了更好地理解这个问题，让我们考虑一个运行多个线程读取和更新相同值的小程序：

```python
from threading import Thread


thread_visits = 0


def visit_counter():
    global thread_visits
    for i in range(100_000):
        value = thread_visits
        thread_visits = value + 1


if __name__ == "__main__":
    thread_count = 100
    threads = [
        Thread(target=visit_counter)
        for _ in range(thread_count)
    ]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()
    print(f"{thread_count=}, {thread_visits=}")
```

上面的程序启动了 100 个线程，每个线程尝试读取和增加 thread_visits 变量 100,000 次。如果我们按顺序运行任务，thread_visits 变量的最终值应该是 10,000,000。但是线程可以交织并导致意想不到的结果。让我们将上面的代码示例保存在 threaded_visits.py 文件中，并运行几次以查看实际结果：

```python
$ python3 thread_visits.py
thread_count=100, thread_visits=6859624
$ python3 thread_visits.py
thread_count=100, thread_visits=7234223
$ python3 thread_visits.py
thread_count=100, thread_visits=7194665
```

在每次运行中，我们得到了一个完全不同的数字，并且总是与预期的 10,000,000 次线程访问相距甚远。但这并不意味着实际的线程访问次数那么少。有了如此多的线程，它们开始交织并影响我们的结果。

这种情况称为比赛危险或比赛条件。它是多线程应用程序中最令人讨厌的软件错误的罪魁祸首之一。显然，在 thread_visits 变量上的读写操作之间有一段时间，另一个线程可以介入并操作结果。

有人可能认为这个问题可以使用 += 运算符来解决，它看起来像一个原子操作：

```python
def visit_counter():
    global thread_visits
    for i in range(100_000):
        thread_visits += 1
```


但这也无济于事！ += 运算符只是增加变量的简写，但它实际上需要在 Python 解释器中进行一些操作。在这些操作之间，线程仍有时间交织。

解决竞争条件的正确方法是使用线程锁定原语。 Python 在线程模块中有一些锁类。这里我们可以使用最简单的一个——threading.Lock。下面是一个线程安全的visit_counter()函数的例子：

```python
from threading import Lock
thread_visits = 0
thread_visits_lock = Lock()


def visit_counter():
    global thread_visits
    for i in range(100_000):
        with thread_visits_lock:
            thread_visits += 1
```

如果你运行修改后的代码版本，你会注意到带锁的线程访问被正确计算。但这将以牺牲性能为代价。线程 Lock() 将确保一次只有一个线程可以处理单个代码块。这意味着受保护的块不能并行运行。此外，获取和释放锁是需要一些额外努力的操作。由于有很多线程试图访问锁，性能下降会很明显。我们将在本章稍后看到使用锁来保护并行数据访问的其他示例。

操作系统内核级别通常支持多线程。当机器具有单核处理器时，系统使用时间切片机制来允许线程看似并行运行。通过时间切片，CPU 从一个线程切换到另一个线程如此之快，以至于产生线程同时运行的错觉。

> 如今，单核 CPU 在台式计算机中非常少见，但在其他领域仍然是一个问题。 许多云计算平台中的小而便宜的实例，以及低成本的嵌入式系统，通常只有单核 CPU 或虚拟 CPU。

没有多个处理单元的并行显然是虚拟的，在这种硬件上的应用程序性能增益更难以评估。 无论如何，有时，使用线程实现代码仍然很有用，即使这意味着必须在单个内核上执行。 我们稍后将审查此类用例。

当你的执行环境具有多个处理器或多个处理器内核时，一切都会发生变化。 在这种情况下，OS 内核可以在 CPU 或其内核之间分配线程。 因此，这提供了显着更快地运行程序的机会。 对于许多编程语言来说都是如此，但对 Python 来说则不一定。 要理解为什么会这样，让我们仔细看看 Python 如何处理线程。

### Python 如何处理多线程

与其他一些语言不同，Python 使用多个内核级线程，可以运行任何解释器级线程。内核级线程由操作系统内核操作和调度。 CPython 使用特定于操作系统的系统调用来创建线程和加入线程。它无法完全控制线程何时运行以及它们将在哪个 CPU 内核上执行。这些责任由系统内核自行决定。此外，内核可以随时抢占正在运行的线程，例如运行具有更高优先级的线程。

不幸的是，Python（CPython 解释器）语言的标准实现有一个主要限制，这使得线程在许多上下文中不太有用。所有访问 Python 对象的操作都由一个全局锁序列化。这样做是因为解释器的许多内部结构不是线程安全的，需要受到保护。并不是每一个操作都需要加锁，在某些情况下线程会释放锁。

> 在并行处理的上下文中，如果我们说某事是串行化的，我们的意思是以串行方式（一个接一个）执行操作。并发程序中的意外序列化通常是我们想要避免的。

CPython 解释器的这种机制被称为全局解释器锁 (GIL)。移除 GIL 是一个偶尔出现在 Python-dev 电子邮件列表中的话题，并且被 Python 开发人员多次假设。遗憾的是，在撰写本文时，还没有人设法提供一种合理且简单的解决方案来让你摆脱这种限制。我们极不可能在短期内看到这方面的任何进展。假设 GIL 将保留在 CPython 中更安全，因此我们需要学习如何使用它。

那么，Python 中的多线程有什么意义呢？当线程仅包含纯 Python 代码并且不执行任何 I/O 操作（例如通过套接字进行通信）时，使用线程来加速程序就没有什么意义了。那是因为 GIL 很可能会全局序列化所有线程的执行。但请记住，GIL 只关心保护 Python 对象。在实践中，GIL 是在许多阻塞系统调用（如套接字调用）上释放的。它也可以在不使用任何 Python/C API 函数的 C 扩展部分中发布。这意味着多个线程可以完全并行地执行 I/O 操作或执行特制的 C 扩展代码。

> 我们将在第 9 章“桥接 Python 与 C 和 C++”中讨论与 Python C 扩展中的 GIL 交互的细节。

多线程允许你在程序等待外部资源时有效地利用时间。这是因为释放 GIL 的休眠线程（这在 CPython 内部发生）可以等待“待机”并在结果返回时“唤醒”。最后，每当程序需要提供响应式界面时，多线程都可以成为解决方案，即使在操作系统需要使用时间切片的单核环境中也是如此。通过多线程，程序可以轻松地与用户交互，同时在所谓的后台进行一些繁重的计算。

> 请注意，GIL 并不存在于 Python 语言的每个实现中。它是 CPython、Stackless Python 和 PyPy 的限制，但在 Jython（用于 JVM 的 Python）和 IronPython（用于 .NET 的 Python）中不存在。还有一些 PyPy 的无 GIL 版本的开发。它基于软件事务内存，称为 PyPy-STM。

在下一节中，我们将讨论线程可能有用的情况的更具体示例。

#### 什么时候应该使用多线程？

尽管有 GIL 限制，线程在以下某些情况下确实很有用：

- 应用程序响应性：应用程序可以接受新输入并在给定时间范围内响应（响应），即使它们没有完成处理先前的输入。
- 多用户应用程序和网络通信：应该同时接受多个用户输入的应用程序通常通过网络与用户进行通信。这意味着他们可以通过利用 CPython 中释放 GIL 的那些部分来大大减少锁定的影响。
- 工作委托和后台处理：大部分繁重工作由外部应用程序或服务完成的应用程序，你的代码充当这些资源的网关。

让我们从响应式应用程序开始，因为这些应用程序比其他并发模型更喜欢多线程。

##### 应用响应能力

假设你要求操作系统通过其图形用户界面将大文件从一个文件夹复制到另一个文件夹。任务可能会被推入后台，界面窗口将显示不断刷新的进度状态。通过这种方式，你可以获得有关整个过程进度的实时反馈。你还可以取消操作。你还可以在操作系统仍在复制文件时执行其他工作，例如浏览网页或编辑文档。你系统的图形用户界面将始终响应你的操作。与原始 cp 或 copy shell 命令相比，这比在整个工作完成之前不提供任何反馈的刺激性小。

响应式界面还允许用户同时处理多个任务。例如，Gimp（一种流行的开源图像编辑应用程序）可以让你在过滤另一张图片的同时处理一张图片，因为这两个任务是独立的。

在尝试实现这样的响应式界面时，一个好的方法是尝试将长时间运行的任务推入后台，或者至少尝试向用户提供持续的反馈。实现这一目标的最简单方法是使用线程。在这种情况下，线程用于确保用户仍然可以操作界面，即使应用程序需要处理其任务更长的时间。

这种方法通常与事件驱动编程一起使用，其中主应用程序线程推送事件以供后台工作线程处理（参见第 7 章，事件驱动编程）。 Web 浏览器是经常使用这种架构模式的应用程序的好例子。

> 不要将应用程序响应性与响应式网页设计 (RDW) 混淆。后者是一种流行的 Web 应用程序设计方法，它允许你在各种媒体（例如桌面浏览器、手机或平板电脑）上很好地显示相同的 Web 应用程序。

##### 多用户应用程序

同时为多个用户提供服务可以理解为应用响应性的一种特殊情况。关键的区别在于，这里的应用程序必须满足许多用户的并行输入，并且每个用户都可能对应用程序的响应速度有一些期望。简而言之，一个用户不应该为了服务而等待其他用户输入被处理。

线程是多用户应用程序的流行并发模型，在 Web 应用程序中极为常见。例如，Web 服务器的主线程可能会接受所有传入连接，但会将每个请求的处理分派给单独的专用线程。这通常允许我们同时处理多个连接和请求。应用程序能够同时处理的连接数和请求数仅受主线程快速接受连接和分派请求到新线程的能力的限制。这种方法的一个限制是使用它的应用程序可能会很快消耗许多资源。线程不是空闲的：内存是共享的，但每个线程至少会分配自己的堆栈。如果线程数太大，总内存消耗很快就会失控。

线程多用户应用程序的另一种模型假设总是有一个有限的线程池充当能够处理传入用户输入的工作线程。然后主线程只负责分配和管理工作线程池。 Web 应用程序也经常使用这种模式。例如，Web 服务器可以创建有限数量的线程，并且这些线程中的每一个都能够自行接受连接并处理通过该连接传入的所有请求。这种方法通常允许你同时为更少的用户提供服务（与每个请求一个线程相比），但可以更好地控制资源使用。两个非常流行的 Python WSGI 兼容 Web 服务器——Gunicorn 和 uWSGI——允许以通常遵循这一原则的方式使用线程工作者来处理 HTTP 请求。

> WSGI 代表 Web 服务器网关接口。它是一种通用的 Python 标准（在 PEP 3333 中定义，可在 https://www.python.org/dev/peps/pep-3333/ 访问）用于 Web 服务器和应用程序之间的通信，促进了 Web 服务器之间 Web 应用程序的可移植性.大多数现代 Python Web 框架和 Web 服务器都基于 WSGI。

使用多线程在多用户应用程序中启用并发通常比使用多进程在资源方面更便宜。单独的 Python 进程将使用比线程更多的内存，因为需要为每个进程加载一个新的解释器。另一方面，拥有太多线程也很昂贵。我们知道 GIL 对于 I/O 密集型应用程序来说不是这样的问题，但总会有需要执行 Python 代码的时候。由于你无法使用裸线程并行化所有应用程序部分，因此你将永远无法利用具有多核 CPU 和单个 Python 进程的机器上的所有资源。这就是为什么有时最佳解决方案是多进程和多线程的混合——多个工作者（进程）与多个线程一起运行。幸运的是，一些符合 WSGI 的 Web 服务器允许进行这样的设置（例如，具有 gthread 工作线程类型的 Gunicorn）。

多用户应用程序通常利用将工作委派给线程作为确保为多个用户提供适当响应的一种手段。但单独的工作委托也可以理解为多线程的独立用例。

##### 工作委托和后台处理
如果你的应用程序依赖于许多外部资源，线程可能真的有助于加快速度。让我们考虑一个函数的情况，该函数为文件夹中的文件编制索引并将构建的索引推送到数据库中。根据文件的类型，该函数执行不同的处理例程。例如，一个专门用于 PDF，另一个专门用于 OpenOffice 文件。

你的函数可以为每个转换器设置一个线程，并通过队列将要完成的作业推送到每个转换器，而不是按顺序处理所有文件。该函数所花费的总时间将更接近于最慢转换器的处理时间，而不是工作的总和。

线程的另一个常见用例是对外部服务执行多个网络请求。例如，如果你想从远程 Web API 获取多个结果，则可能需要大量时间来同步执行此操作，尤其是在远程服务器位于远处的情况下。

如果你在发出新请求之前等待所有先前的响应，那么你将花费大量时间来等待外部服务响应。额外的往返时间延迟将添加到每个此类请求中。

如果你正在与一些高效的服务（例如 Google Maps API）进行通信，它很可能可以同时处理你的大部分请求，而不会影响单个请求的响应时间。然后在单独的线程中执行多个查询是合理的。在这里，当执行 HTTP 请求时，应用程序很可能会花费大部分时间从 TCP 套接字读取。将此类工作委托给线程可以极大地提高应用程序的性能。

### 多线程应用程序示例

为了了解 Python 线程在实践中是如何工作的，让我们构建一个可以从线程使用中受益的示例应用程序。我们将考虑一个在上一节中已经强调的简单问题，作为多线程的一个常见用例：向某些远程服务发出并行 HTTP 请求。

假设我们需要使用多个查询从某个 Web 服务中获取信息，而这些查询无法批量处理为单个批量 HTTP 请求。作为一个现实的例子，我们将使用来自 https://www.vatcomply.com 的免费 API 中的外汇参考汇率端点。这种选择的原因如下：

- 此服务是开放的，不需要任何身份验证密钥。
- 该服务的接口非常简单，可以使用流行的请求库轻松查询。
- 此 API 使用在许多类似 API 中通用的货币数据格式。如果此服务出现故障（或停止免费），你将能够轻松地将 API 的基本 URL 切换到其他服务的 URL。

> 免费的 API 服务来来去去。有可能一段时间后本书中的 URL 将无法使用，或者 API 将需要付费订阅。在这种情况下，运行你自己的服务可能是一个不错的选择。
>
> 在 https://github.com/exchangeratesapi/exchangeratesapi，你可以找到货币兑换 API 服务的代码，该服务使用与本章使用的 API 相同的数据格式。

在我们的示例中，我们将尝试使用多种货币作为参考点来获取选定货币的汇率。然后，我们将结果以汇率货币矩阵的形式呈现，类似于以下内容：

```python
1 USD =    1.0 USD,  0.887 EUR,    3.8 PLN,   8.53 NOK,  22.7 CZK
1 EUR =   1.13 USD,    1.0 EUR,   4.29 PLN,   9.62 NOK,  25.6 CZK
1 PLN =  0.263 USD,  0.233 EUR,    1.0 PLN,   2.24 NOK,  5.98 CZK
1 NOK =  0.117 USD,  0.104 EUR,  0.446 PLN,    1.0 NOK,  2.66 CZK
1 CZK =  0.044 USD,  0.039 EUR,  0.167 PLN,  0.375 NOK,   1.0 CZK
```

我们选择的 API 提供了多种方法来查询单个请求中的多个数据点，但遗憾的是它不允许你同时使用多种基础货币查询数据。 获取单个碱基的速率与执行以下操作一样简单：

```python
>>> import requests
>>> response = requests.get("https://api.vatcomply.com/rates?base=USD")
>>> response.json()
{'base': 'USD', 'rates': {'BGN': 1.7343265053, 'NZD': 1.4824864769, 'ILS': 3.5777245721, 'RUB': 64.7361000266, 'CAD': 1.3287221779, 'USD': 1.0, 'PHP': 52.0368892436, 'CHF': 0.9993792675, 'AUD': 1.3993970027, 'JPY': 111.2973308504, 'TRY': 5.6802341048, 'HKD': 7.8425113062, 'MYR': 4.0986077858, 'HRK': 6.5923561231, 'CZK': 22.7170346723, 'IDR': 14132.9963642813, 'DKK': 6.6196683515, 'NOK': 8.5297508203, 'HUF': 285.09355325, 'GBP': 0.7655848187, 'MXN': 18.930477964, 'THB': 31.7495787887, 'ISK': 118.6485767491, 'ZAR': 14.0298838344, 'BRL': 3.8548372794, 'SGD': 1.3527533919, 'PLN': 3.8015429636, 'INR': 69.3340427419, 'KRW': 1139.4519819101, 'RON': 4.221867518, 'CNY': 6.7117141084, 'SEK': 9.2444799149, 'EUR': 0.8867606633}, 'date': '2019-04-09'}
```

> 为了保持我们的示例简洁，我们将使用 requests 包来执行 HTTP 请求。 它不是标准库的一部分，但可以使用 pip 从 PyPI 轻松获取。
>
> 你可以在 https://requests.readthedocs.io/ 阅读有关请求的更多信息。

由于我们的目标是展示并发问题的多线程解决方案与经典同步解决方案的比较，我们将从根本不使用线程的实现开始。 这是一个程序的代码，它循环遍历基础货币列表，查询外汇汇率 API，并将结果显示在标准输出中作为文本格式的表格：

```python
import time
import requests


SYMBOLS = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
BASES = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')


def fetch_rates(base):
    response = requests.get(
        f"https://api.vatcomply.com/rates?base={base}"
    )
    response.raise_for_status()
    rates = response.json()["rates"]
    
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    rates_line = ", ".join(
        [f"{rates[symbol]:7.03} {symbol}" for symbol in SYMBOLS]
    )
    print(f"1 {base} = {rates_line}")


def main():
    for base in BASES:
        fetch_rates(base)


if __name__ == "__main__":
    started = time.time()
    main()
    elapsed = time.time() - started
    print()
    print("time elapsed: {:.2f}s".format(elapsed))
```

main() 函数遍历基础货币列表并调用 fetch_rates() 函数以获取基础货币的汇率。在 fetch_rates() 中，我们使用 requests.get() 函数发出单个 HTTP 请求。如果服务器返回带有表示服务器或客户端错误的状态代码的响应，则 response.raise_for_status() 方法将引发异常。目前，我们不希望有任何异常，只是假设在收到请求后，我们可以使用 response.json() 方法成功读取响应负载。我们将在处理线程中的错误部分讨论如何正确处理线程中引发的异常。

我们在 main() 函数的执行周围添加了一些语句，旨在测量完成工作所需的时间。让我们将该代码保存在一个名为 synchronous.py 的文件中并执行它以查看它是如何工作的：

```python
$ python3 synchronous.py
```

在我的计算机上，完成该任务可能需要几秒钟：

```python
1 USD =     1.0 USD,   0.823 EUR,    3.73 PLN,     8.5 NOK,    21.5 CZK
1 EUR =    1.22 USD,     1.0 EUR,    4.54 PLN,    10.3 NOK,    26.2 CZK
1 PLN =   0.268 USD,    0.22 EUR,     1.0 PLN,    2.28 NOK,    5.76 CZK
1 NOK =   0.118 USD,  0.0968 EUR,   0.439 PLN,     1.0 NOK,    2.53 CZK
1 CZK =  0.0465 USD,  0.0382 EUR,   0.174 PLN,   0.395 NOK,     1.0 CZK
time elapsed: 4.08s
```

每次运行我们的脚本总是需要不同的时间。这是因为处理时间主要取决于可通过网络连接访问的远程服务。影响最终结果的不确定因素很多。如果我们想要真正有条不紊，我们会进行更长的测试，重复多次，然后根据测量值计算平均值。但为了简单起见，我们不会这样做。稍后你将看到，这种简化的方法仅用于说明目的。

我们有一些基线实现。现在是介绍线程的时候了。在下一节中，我们将尝试在每次调用 fetch_rates() 函数时引入一个线程。

#### 每个项目使用一个线程

现在，是时候改进了。我们在 Python 中没有做很多进程，执行时间长是与外部服务的通信造成的。我们向远程服务器发送一个 HTTP 请求，它计算答案，然后我们等待直到响应传回。

涉及很多 I/O，因此多线程似乎是一个可行的选择。我们可以在单独的线程中一次启动所有请求，然后等待我们从所有请求接收数据。如果我们与之通信的服务能够并发处理我们的请求，我们肯定会看到性能改进。

所以，让我们从最简单的方法开始。 Python 使用 threading 模块为系统线程提供了干净且易于使用的抽象。这个标准库的核心是 Thread 类，它表示单个线程实例。这是 main() 函数的修改版本，它为每个要处理的基础货币创建并启动一个新线程，然后等待所有线程完成：

```python
from threading import Thread


def main():
    threads = []
    for base in BASES:
        thread = Thread(target=fetch_rates, args=[base])
        thread.start()
        threads.append(thread)
    while threads:
        threads.pop().join()
```

这是一种快速而肮脏的解决方案，以一种轻浮的方式解决问题。它有一些严重的问题，我们将在稍后解决。但是，嘿，它有效。我们把修改后的脚本保存在threads_one_per_item.py文件中，运行一下看看性能有没有提升：

```python
$ python3 one_thread_per_item.py
```

在我的计算机上，我看到总处理时间有了显着改善：

```python
1 EUR =    1.22 USD,     1.0 EUR,    4.54 PLN,    10.3 NOK,    26.2 CZK
1 NOK =   0.118 USD,  0.0968 EUR,   0.439 PLN,     1.0 NOK,    2.53 CZK
1 CZK =  0.0465 USD,  0.0382 EUR,   0.174 PLN,   0.395 NOK,     1.0 CZK
1 USD =     1.0 USD,   0.823 EUR,    3.73 PLN,     8.5 NOK,    21.5 CZK
1 PLN =   0.268 USD,    0.22 EUR,     1.0 PLN,    2.28 NOK,    5.76 CZK
time elapsed: 1.34s
```

> 由于在线程内部使用了 print()，你将看到的输出可能会略有格式错误。这是我们将在本节后面处理的多线程问题之一。

一旦我们知道线程对我们的应用程序有有益的影响，就该以更合乎逻辑的方式使用它们了。首先，我们需要确定上述代码中存在的以下问题：

- 我们为每个参数启动一个新线程。线程初始化也需要一些时间，但这个小开销并不是唯一的问题。线程还消耗其他资源，如内存或文件描述符。我们的示例输入有严格定义的项目数，但如果没有限制呢？你绝对不想运行依赖于任意大小的数据输入的无限数量的线程。
- 在线程中执行的 fetch_rates() 函数调用内置的 print() 函数，实际上你不太可能希望在主应用程序线程之外执行此操作。这主要是由于标准输出在 Python 中的缓冲方式。当对该函数的多次调用在线程之间交织时，你可能会遇到格式错误的输出。此外，print() 函数被认为很慢。如果在多线程中鲁莽使用，它可能会导致序列化，这将浪费多线程的所有好处。
- 最后但并非最不重要的一点是，通过将每个函数调用委托给一个单独的线程，我们很难控制处理输入的速率。是的，我们希望尽可能快地完成这项工作，但很多时候，外部服务对它们可以处理的来自单个客户端的请求速率进行硬限制。有时，以一种使你能够限制处理速度的方式设计程序是合理的，这样你的应用程序就不会因滥用其使用限制而被外部 API 列入黑名单。

在下一节中，我们将看到如何使用线程池来解决无限线程数的问题。

#### 使用线程池

我们将尝试解决的第一个问题是由我们的程序运行的无限数量的线程。一个好的解决方案是构建一个严格定义大小的线程工作池，它将处理所有并行工作并通过一些线程安全数据结构与主线程通信。通过使用这种线程池方法，我们还可以更轻松地解决我们在上一节中提到的另外两个问题。

总体思路是启动预定义数量的线程，这些线程将消耗队列中的工作项，直到队列变空。当没有其他工作要做时，线程会退出，我们就可以退出程序了。我们的通信数据结构的一个很好的候选者是来自内置队列模块的 Queue 类。它是一种先进先出 (FIFO) 队列实现，与来自 collections 模块的 deque 集合非常相似，专门设计用于处理线程间通信。这是 main() 函数的修改版本，它仅启动有限数量的工作线程，并将新的 worker() 函数作为目标，并使用线程安全队列与它们通信：

```python
from queue import Queue
from threading import Thread
THREAD_POOL_SIZE = 4
def main():
    work_queue = Queue()
    for base in BASES:
        work_queue.put(base)
    threads = [
        Thread(target=worker, args=(work_queue,))
        for _ in range(THREAD_POOL_SIZE)
    ]
    for thread in threads:
        thread.start()
    work_queue.join()
    while threads:
        threads.pop().join()
```

> Python 有一些内置的线程池实用程序。我们稍后将在使用 multiprocessing.dummy 作为多线程接口部分中介绍它们。

main 函数将 Queue 实例初始化为 worker_queue 变量，并将所有基础货币作为工作项放入队列中以供工作线程处理。然后，它使用 worker() 函数作为线程目标和 work_queue 作为它们的输入参数来初始化 THREAD_POOL_SIZE 数量的线程。然后等待，直到使用 work_queue.join() 处理完所有项目，然后通过调用每个 Thread 实例的 join 方法等待所有线程完成。

队列中工作项的处理发生在工作函数中。这是它的代码：

```python
from queue import Empty


def worker(work_queue):
    while not work_queue.empty():
        try:
            item = work_queue.get_nowait()
        except Empty:
            break
        else:
            fetch_rates(item)
            work_queue.task_done()
```

worker() 函数在 while 循环中运行，直到 work_queue.empty() 返回 True。在每次迭代中，它尝试使用 work_queue.get_nowait() 方法以非阻塞方式获取新项目。如果队列已经为空，它将引发一个 Empty 异常，我们的函数将中断循环并完成。如果有一个项目要从队列中选择，worker() 函数会将它传递给 fetch_rates(item) 并使用 work_queue.task_done() 将该项目标记为已处理。当队列中的所有项目都被标记为完成时，主线程的 work_queue.join() 函数将返回。

脚本的其余部分，即 fetch_rates() 函数和 if \_\_name\_\_ == "\_\_main\_\_" 子句下的代码保持不变。以下是我们可以保存在 thread_pool.py 文件中的完整脚本：

```python
import time
from queue import Queue, Empty
from threading import Thread
import requests


THREAD_POOL_SIZE = 4
SYMBOLS = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
BASES = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')


def fetch_rates(base):
    response = requests.get(
        f"https://api.vatcomply.com/rates?base={base}"
    )
    response.raise_for_status()
    rates = response.json()["rates"]
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    rates_line = ", ".join(
        [f"{rates[symbol]:7.03} {symbol}" for symbol in SYMBOLS]
    )
    print(f"1 {base} = {rates_line}")


def worker(work_queue):
    while not work_queue.empty():
        try:
            item = work_queue.get_nowait()
        except Empty:
            break
        else:
            fetch_rates(item)
            work_queue.task_done()


def main():
    work_queue = Queue()
    for base in BASES:
        work_queue.put(base)
    threads = [
        Thread(target=worker, args=(work_queue,))
        for _ in range(THREAD_POOL_SIZE)
    ]
    for thread in threads:
        thread.start()
    work_queue.join()
    while threads:
        threads.pop().join()


if __name__ == "__main__":
    started = time.time()
    main()
    elapsed = time.time() - started
    print()
    print("time elapsed: {:.2f}s".format(elapsed))
```

我们现在可以执行脚本以查看与之前的尝试是否有任何性能差异：

```python
python thread_pool.py
```

在我的电脑上，我可以看到以下输出：

```python
1 NOK =   0.118 USD,  0.0968 EUR,   0.439 PLN,     1.0 NOK,    2.53 CZK
1 PLN =   0.268 USD,    0.22 EUR,     1.0 PLN,    2.28 NOK,    5.76 CZK
1 USD =     1.0 USD,   0.823 EUR,    3.73 PLN,     8.5 NOK,    21.5 CZK
1 EUR =    1.22 USD,     1.0 EUR,    4.54 PLN,    10.3 NOK,    26.2 CZK
1 CZK =  0.0465 USD,  0.0382 EUR,   0.174 PLN,   0.395 NOK,     1.0 CZK
time elapsed: 1.90s
```

整体运行时间可能比每个参数使用一个线程时慢，但至少现在不可能用任意长的输入耗尽所有计算资源。此外，我们可以调整 THREAD_POOL_SIZE 参数以获得更好的资源/时间平衡。

在这次尝试中，我们使用了 fetch_rates() 函数的未修改版本，该函数直接从线程内部在标准输出上输出 API 结果。在某些情况下，当两个线程尝试同时打印结果时，这可能会导致格式错误的输出。在下一节中，我们将尝试通过引入双向队列来改进它。

#### 使用双向队列

我们现在能够解决的问题是在线程中打印输出的潜在问题。将这样的责任留给启动工作线程的主线程会好得多。我们可以通过提供另一个队列来处理这个问题，该队列将负责从我们的工作人员那里收集结果。这是将所有内容放在一起的完整代码，突出显示了主要更改：

```python
import time
from queue import Queue, Empty
from threading import Thread
import requests


SYMBOLS = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
BASES = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
THREAD_POOL_SIZE = 4


def fetch_rates(base):
    response = requests.get(
        f"https://api.vatcomply.com/rates?base={base}"
    )
    response.raise_for_status()
    rates = response.json()["rates"]
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    return base, rates


def present_result(base, rates):
    rates_line = ", ".join([
        f"{rates[symbol]:7.03} {symbol}" 
        for symbol in SYMBOLS
    ])
    print(f"1 {base} = {rates_line}")


def worker(work_queue, results_queue):
    while not work_queue.empty():
        try:
            item = work_queue.get_nowait()
        except Empty:
            break
        else:
            results_queue.put(fetch_rates(item))
            work_queue.task_done()


def main():
    work_queue = Queue()
    results_queue = Queue()
    for base in BASES:
        work_queue.put(base)
    threads = [
        Thread(
            target=worker, 
            args=(work_queue, results_queue)
        ) for _ in range(THREAD_POOL_SIZE)
    ]
    for thread in threads:
        thread.start()
    work_queue.join()
    while threads:
        threads.pop().join()
    while not results_queue.empty():
        present_result(*results_queue.get())


if __name__ == "__main__":
    started = time.time()
    main()
    elapsed = time.time() - started
    print()
    print("time elapsed: {:.2f}s".format(elapsed))
```

主要区别是引入了Queue类的results_queue实例和presents_results()函数。 fetch_rates() 函数不再将其结果打印到标准输出。相反，它将处理后的 API 结果直接返回给 worker() 函数。工作线程通过新的 results_queue 输出队列未经修改地传递这些结果。

现在只有主线程负责在标准输出上打印结果。在所有工作都被标记为完成后，main() 函数会使用 results_queue 的结果并将它们传递给 present_results() 函数。

这消除了如果present_result() 函数执行更多print() 时我们可能遇到的格式错误输入的风险。我们不希望这种方法在小输入下有任何性能改进，但实际上我们还降低了由于 print() 执行缓慢而导致线程序列化的风险。

在前面的所有示例中，我们假设我们使用的 API 将始终以有意义且有效的答案进行响应。为了简单起见，我们没有涵盖任何故障场景，但在实际应用中，这可能是一个问题。在下一节中，我们将看到在线程中引发异常时会发生什么以及它如何影响队列通信。

#### 处理线程中的错误

如果 HTTP 响应具有指示错误条件的状态代码，则 requests.Response 对象的 raise_for_status() 方法将引发异常。我们在 fetch_rates() 函数的所有先前迭代中都使用了该方法，但我们还没有处理任何潜在的异常。

如果我们使用 requests.get() 方法调用的服务以指示错误的状态代码响应，则异常将在单独的线程中引发并且不会使整个程序崩溃。工作线程当然会立即退出。但是主线程将等待所有存储在 work_queue 上的任务完成（使用 work_queue.join() 调用）。如果不进一步改进，我们最终可能会遇到一些工作线程崩溃并且程序永远不会退出的情况。为了避免这种情况，我们应该确保我们的工作线程优雅地处理可能的异常，并确保队列中的所有项目都得到处理。

让我们对我们的代码进行一些小的更改，以便为可能发生的任何问题做好准备。如果工作线程中出现异常，我们可能会在 results_queue 队列中放置一个错误实例，以便主线程能够判断哪些任务处理失败。我们还可以将当前任务标记为已完成，就像没有错误时所做的那样。这样，我们确保主线程在等待 work_queue.join() 方法调用时不会无限期锁定。

然后主线程可能会检查结果并重新引发在结果队列中发现的任何异常。以下是 worker() 和 main() 函数的改进版本，它们可以以更安全的方式处理异常（突出显示了更改）：

```python
def worker(work_queue, results_queue):
    while not work_queue.empty():
        try:
            item = work_queue.get_nowait()
        except Empty:
            break
        
        try:
            result = fetch_rates(item)
        except Exception as err:
            results_queue.put(err)
        else:
            results_queue.put(result)
        finally:
            work_queue.task_done()


def main():
    work_queue = Queue()
    results_queue = Queue()
    for base in BASES:
        work_queue.put(base)
    threads = [
        Thread(target=worker, args=(work_queue, results_queue))
        for _ in range(THREAD_POOL_SIZE)
    ]
    for thread in threads:
        thread.start()
    work_queue.join()
    while threads:
        threads.pop().join()
    while not results_queue.empty():
        result = results_queue.get()
        if isinstance(result, Exception):
            raise result
        present_result(*result)
```

为了了解错误处理是如何运作的，我们将尝试模拟一个令人信服的错误场景。由于我们无法完全控制所使用的 API，因此我们将随机向 fetch_rates() 函数注入错误响应。以下是该函数的修改版本：

```python
import random


def fetch_rates(base):
    response = requests.get(
        f"https://api.vatcomply.com/rates?base={base}"
    )
    if random.randint(0, 5) < 1:
        # simulate error by overriding status code
        response.status_code = 500
    response.raise_for_status()
    rates = response.json()["rates"]
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    return base, rates
```

通过将 response.status_code 修改为 500，我们将模拟我们的 API 返回指示服务器错误的响应的情况。这是服务器端发生的问题的常见状态代码。在这种情况下，并不总是披露错误的详细信息。此状态代码足以让 response.raise_for_status() 方法引发异常。

让我们在 error_handling.py 文件中保存代码的修改版本并运行它以查看它如何处理异常：

```python
$ python3 error_handling.py
```

错误是随机注入的，因此这可能需要执行几次。几次尝试后，你应该会看到类似于以下内容的输出：

```python
1 PLN =   0.268 USD,    0.22 EUR,     1.0 PLN,    2.28 NOK,    5.76 CZK
Traceback (most recent call last):
  File ".../error_handling.py", line 92, in <module>
    main()
  File ".../error_handling.py", line 85, in main
    raise result
  File ".../error_handling.py", line 53, in worker
    result = fetch_rates(item)
  File ".../error_handling.py", line 30, in fetch_rates
    response.raise_for_status()
  File ".../.venv/lib/python3.9/site-packages/requests/models.py", line 943, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: OK for url: https://api.vatcomply.com/rates?base=NOK
```

我们的代码没有成功获取所有项目，但我们至少得到了关于错误原因的明确信息，在这种情况下是 500 Server Error 响应状态。

在下一节中，我们将对我们的多线程程序进行最后的改进。我们将引入限制机制来保护我们的程序不受速率限制并避免我们使用的免费服务被意外滥用。

#### 节流

我们尚未解决的每个项目使用一个线程部分中提到的最后一个问题是外部服务提供商可能施加的潜在速率限制。对于外汇汇率 API，服务维护者没有通知我们任何速率限制或节流机制。但是许多服务（甚至是付费服务）通常会施加速率限制。

通常，当服务实施了速率限制时，它会在发出一定数量的请求后开始返回指示错误的响应，超过分配的配额。我们已经在上一节中为错误响应做好了准备，但这通常不足以正确处理速率限制。这是因为许多服务经常计算超出限制的请求，如果你一直超出限制，你可能永远无法回到分配的配额。

使用多线程时，很容易耗尽任何速率限制，或者简单地（如果服务不限制传入请求）使服务饱和到无法响应任何人的水平。如果故意这样做，这被称为拒绝服务 (DoS) 攻击。

为了不超过速率限制或导致意外的 DoS，我们需要限制向远程服务发出请求的速度。限制工作节奏通常称为节流。 PyPI 中有一些包允许你限制任何类型的工作的速度，这些工作非常易于使用。但我们不会在这里使用任何外部代码。节流是为线程引入一些锁定原语的好机会，因此我们将尝试从头开始构建节流解决方案。

我们将使用的算法有时称为令牌桶，非常简单。它包括以下功能：

- 有一个带有预定义令牌数量的存储桶
- 每个令牌对应于处理一项工作的单个权限
- 每次工作人员请求一个或多个令牌（权限）时，我们执行以下操作：
    - 我们检查自上次重新装满桶以来已经过去了多长时间
    - 如果时间差允许，我们用与时间差相对应的令牌数量重新填充桶
    - 如果存储的代币数量大于或等于请求的数量，我们减少存储的代币数量并返回该值
    - 如果存储的令牌数量少于请求的数量，我们返回零

两个重要的事情是始终使用零令牌初始化令牌桶，并且永远不允许它过满。这可能是违反直觉的，但如果我们不遵循这些预防措施，我们可能会以超过速率限制的突发方式释放代币。因为，在我们的情况下，速率限制以每秒请求数表示，我们不需要处理任意的时间量。我们假设我们的测量基数是一秒，所以我们永远不会存储超过该时间允许的请求数量的令牌。这是允许使用令牌桶算法进行节流的类的示例实现：

```python
from threading import Lock 
 
    
class Throttle: 
    def __init__(self, rate): 
        self._consume_lock = Lock() 
        self.rate = rate 
        self.tokens = 0 
        self.last = None
 
    def consume(self, amount=1): 
        with self._consume_lock: 
            now = time.time() 
             
            # time measurement is initialized on first 
            # token request to avoid initial bursts 
            if self.last is None: 
                self.last = now 
 
            elapsed = now - self.last 
 
            # make sure that quant of passed time is big 
            # enough to add new tokens 
            if elapsed * self.rate > 1: 
                self.tokens += elapsed * self.rate
                self.last = now 
 
            # never over-fill the bucket 
            self.tokens = min(self.rate, self.tokens)
 
            # finally dispatch tokens if available 
            if self.tokens >= amount: 
                self.tokens -= amount
                return amount
            
            return 0
```

这个类的用法很简单。我们只需要在主线程中创建一个 Throttle 实例（例如 Throttle(10)），并将其作为位置参数传递给每个工作线程：

```python
def main():
    work_queue = Queue()
    results_queue = Queue()
    throttle = Throttle(10)
    for base in BASES:
        work_queue.put(base)
    threads = [
        Thread(
            target=worker, 
            args=(work_queue, results_queue, throttle)
        ) for _ in range(THREAD_POOL_SIZE)
    ]
    ...
```

这个节流实例将在线程之间共享，但使用它是安全的，因为我们使用线程模块中的 Lock 类的实例来保护其内部状态的操作。我们现在可以更新 worker() 函数实现以等待每个项目直到油门对象释放新令牌，如下所示：

```python
import time


def worker(work_queue, results_queue, throttle): 
    while True: 
        try: 
            item = work_queue.get_nowait() 
        except Empty: 
            break 
        
        while not throttle.consume():
            time.sleep(0.1) 
 
        try: 
            result = fetch_rates(item) 
        except Exception as err: 
            results_queue.put(err) 
        else: 
            results_queue.put(result) 
            finally: 
                work_queue.task_done()
```

如果节流对象不释放任何令牌（零评估为 False），while not Throttle.consume() 块会阻止我们处理工作队列项。我们进行了短暂的睡眠，以便在出现空桶时为线程添加一些步调。可能有一种更优雅的方法来做到这一点，但这种简单的技术可以很好地完成这项工作。

当throttle.consume() 返回一个非零值时，我们考虑消耗的令牌。线程可以退出 while 循环并继续处理工作队列项。处理完成后，它将从工作队列中读取另一个项目，并再次尝试使用令牌。整个过程将继续，直到工作队列为空。

这是对线程的一个非常简短的介绍。我们还没有涵盖多线程应用程序的所有可能方面，但我们已经了解了足够多的知识，可以查看其他并发模型并了解它们与线程的比较。下一个并发模型将是多进程。

## 多进程

老实说，多线程具有挑战性。与同步方法相比，以理智和安全的方式处理线程需要大量的代码。我们必须设置线程池和通信队列，优雅地处理来自线程的异常，并且在尝试提供速率限制功能时还要担心线程安全。只需并行执行某个外部库中的一个函数就需要数十行代码！我们依赖外部包创建者的承诺，即他们的库是线程安全的。对于实际上仅适用于执行 I/O 绑定任务的解决方案来说，听起来价格很高。

允许你实现并行性的另一种方法是多进程。不使用 GIL 相互约束的单独 Python 进程允许更好地利用资源。这对于在多核处理器上运行且执行真正 CPU 密集型任务的应用程序尤其重要。目前，这是唯一可供 Python 开发人员使用的内置并发解决方案（使用 CPython 解释器），可让你在各种情况下从多个处理器内核中受益。

使用多个进程而不是线程的另一个优点是它们不共享内存上下文。因此，更难破坏数据并在应用程序中引入死锁或竞争条件。不共享内存上下文意味着你需要做一些额外的努力来在不同的进程之间传递数据，但幸运的是，有许多好方法可以实现可靠的进程间通信。事实上，Python 提供了一些原语，使进程之间的通信几乎和线程之间的通信一样容易。

在任何编程语言中启动新进程的最基本方法通常是在某个时候分叉程序。在 POSIX 和类似 POSIX 的系统（如 UNIX、macOS 和 Linux）上，fork 是一个系统调用，它将创建一个新的子进程。在 Python 中，它通过 os.fork() 函数公开。这两个进程在分叉后继续各自的程序。这是一个示例脚本，它只派生一次：

```python
import os 


pid_list = []


def main(): 
    pid_list.append(os.getpid()) 
    child_pid = os.fork() 
 
    if child_pid == 0: 
        pid_list.append(os.getpid()) 
        print() 
        print("CHLD: hey, I am the child process") 
        print("CHLD: all the pids I know %s" % pid_list) 
 
    else: 
        pid_list.append(os.getpid()) 
        print() 
        print("PRNT: hey, I am the parent process") 
        print("PRNT: the child pid is %d" % child_pid) 
        print("PRNT: all the pids I know %s" % pid_list) 
 
 
if __name__ == "__main__": 
    main()
```

os.fork() 产生了一个新进程。在 fork() 调用之前，两个进程都将具有相同的内存状态，但在那之后，它们的内存发生分歧，因此有了 fork 名称。 os.fork() 返回一个整数值。如果为0，我们就知道当前进程是子进程。父进程将接收其子进程的进程 ID (PID) 编号。

让我们将脚本保存在 forks.py 文件中并在 shell 会话中运行它：

```python
$ python3 forks.py
```

在我的电脑上，我有以下输出：

```python
PRNT: hey, I am the parent process
PRNT: the child pid is 9304
PRNT: all the pids I know [9303, 9303]
CHLD: hey, I am the child process
CHLD: all the pids I know [9303, 9304]
```

注意两个进程在 os.fork() 调用之前如何具有完全相同的数据初始状态。它们都具有与 pid_list 集合的第一个值相同的 PID 编号（进程标识符）。

后来，这两个状态发生了分歧。我们可以看到子进程添加了 9304 值，而父进程复制了它的 9303 PID。这是因为这两个进程的内存上下文没有共享。它们具有相同的初始条件，但在 os.fork() 调用后不能相互影响。

在 fork 之后，每个进程都有自己的地址空间。为了进行通信，进程需要使用系统范围的资源或使用信号等低级工具。

不幸的是，os.fork 在 Windows 下不可用，需要生成新的解释器来模拟 fork 功能。因此，多进程实现取决于平台。 os 模块还公开了允许你在 Windows 下生成新进程的函数。 Python 提供了很棒的多进程模块，它为多进程创建了一个高级接口。

multiprocessing 模块的巨大优势在于它提供了一些我们在讨论多线程时必须从头开始编码的抽象。它允许你限制样板代码的数量，从而提高应用程序的可维护性并降低复杂性。令人惊讶的是，尽管它的名字是多进程模块，但它为线程公开了一个类似的接口，因此你可能希望对这两种方法使用相同的接口。

让我们在下一节中仔细看看内置的多进程模块。

### 内置多进程模块

multiprocessing 模块提供了一种可移植的方式来处理进程，就好像它们是线程一样。该模块包含一个与Thread类非常相似的Process类，可以在任何平台上使用，如下：

```python
from multiprocessing import Process
import os


def work(identifier):
    print(
        f'Hey, I am the process '
        f'{identifier}, pid: {os.getpid()}'
    )


def main():
    processes = [
        Process(target=work, args=(number,))
        for number in range(5)
    ]
    for process in processes:
        process.start()
    while processes:
        processes.pop().join()


if __name__ == "__main__":
    main()
```

Process 类具有 start() 和 join() 方法，它们类似于 Thread 类中的方法。 start() 方法产生一个新进程，join() 等待子进程退出。

让我们将该脚本保存在一个名为 basic_multiprocessing.py 的文件中并执行它以查看它是如何工作的：

```python
$ python3 basic_multiprocessing.py
```

在你自己的计算机上，你将能够看到类似于以下内容的输出：

```python
Hey, I am the process 3, pid: 9632
Hey, I am the process 1, pid: 9630
Hey, I am the process 2, pid: 9631
Hey, I am the process 0, pid: 9629
Hey, I am the process 4, pid: 9633
```

创建进程时，内存会被分叉（在 POSIX 和类似 POSIX 的系统上）。除了复制的内存状态之外，Process 类还在其构造函数中提供了一个额外的 args 参数，以便可以传递数据。

进程之间的通信需要一些额外的工作，因为它们的本地内存默认不共享。为了缓解这种情况，多进程模块提供了以下几种进程间通信方式：

- 使用 multiprocessing.Queue 类，它是 queue.Queue 的功能等价物，它早先用于线程之间的通信。
- 使用 multiprocessing.Pipe，这是一个类似套接字的双向通信通道。
- 使用 multiprocessing.sharedctypes 模块，它允许你在进程之间共享的专用内存池中创建任意 C 类型（来自 ctypes 模块）。

multiprocessing.Queue 和 queue.Queue 类具有相同的接口。唯一的区别是第一个设计用于多进程环境，而不是多线程，因此它使用不同的内部传输和锁定原语。我们已经在多线程部分看到了如何将队列与多线程一起使用，所以我们不会对多进程做同样的事情。用法保持完全相同，因此这样的示例不会带来任何新内容。

Pipe 类提供了一种更有趣的通信模式。它是一个双工（双向）通信通道，在概念上与 UNIX 管道非常相似。 Pipe 的接口非常类似于来自内置套接字模块的简单套接字。原始系统管道和套接字之间的区别在于它通过 pickle 模块自动应用对象序列化。从开发人员的角度来看，它看起来像发送普通的 Python 对象。对于普通的系统管道或套接字，你需要手动应用你自己的序列化，以便从字节流重建发送的对象。

> pickle 模块可以轻松地将 Python 对象与字节流进行序列化和反序列化。它处理各种类型的对象，包括用户定义类的实例。你可以在 https://docs.python.org/3/library/pickle.html 上了解更多关于 pickle 模块以及哪些对象可以pickle。

这允许进程之间更轻松的通信，因为你几乎可以发送任何基本的 Python 类型。考虑以下 worker() 类，它将从 Pipe 对象中读取一个对象并在标准输出上输出其表示：

```python
def worker(connection):
    while True:
        instance = connection.recv()
        if instance:
            print(f"CHLD: recv: {instance}")
        if instance is None:
            break
```

稍后，我们可以在 main() 函数中使用 Pipe 将各种对象（包括自定义类）发送到子进程：

```python
from multiprocessing import Process, Pipe 
 
class CustomClass: 
    pass 
 
def main():
    parent_conn, child_conn = Pipe()
    child = Process(target=worker, args=(child_conn,))
    for item in (
        42,
        'some string',
        {'one': 1},
        CustomClass(),
        None,
    ):
        print(
            "PRNT: send: {}".format(item)
        )
        parent_conn.send(item)
    child.start()
    child.join()


if __name__ == "__main__": 
    main()
```

查看前面脚本的以下示例输出时，你将看到可以轻松传递自定义类实例，并且它们具有不同的地址，具体取决于流程：

```python
PRNT: send: 42
PRNT: send: some string
PRNT: send: {'one': 1}
PRNT: send: <__main__.CustomClass object at 0x101cb5b00>
PRNT: send: None
CHLD: recv: 42
CHLD: recv: some string
CHLD: recv: {'one': 1}
CHLD: recv: <__main__.CustomClass object at 0x101cba400>
```

在进程之间共享状态的另一种方法是在共享内存池中使用原始类型，以及 multiprocessing.sharedctypes 中提供的类。最基本的是值和数组。以下是多进程模块官方文档中的一些示例代码：

```python
from multiprocessing import Process, Value, Array 
def f(n, a): 
    n.value = 3.1415927 
    for i in range(len(a)): 
        a[i] = -a[i] 
 
 
if __name__ == '__main__': 
    num = Value('d', 0.0) 
    arr = Array('i', range(10)) 
 
    p = Process(target=f, args=(num, arr)) 
    p.start() 
    p.join() 
 
    print(num.value) 
    print(arr[:])
```

此示例将打印以下输出：

```python
3.1415927
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
```

在使用 multiprocessing.sharedctypes 时，你需要记住你正在处理共享内存，因此为了避免竞争条件的风险，你仍然需要使用锁定原语。 multiprocessing 提供了一些类似于线程模块中可用的类，例如 Lock、RLock 和 Semaphore。 sharedctypes 类的缺点是它们只允许你共享 ctypes 模块中的基本 C 类型。如果需要传递更复杂的结构或类实例，则需要改用队列、管道或其他进程间通信通道。在大多数情况下，避免共享类型中的类型是合理的，因为它们会增加代码复杂性并带来多线程的所有危险。

我们已经提到，由于一些额外的功能，多进程模块允许你减少样板的数量。一种这样的功能是内置的进程池。我们将在下一节中了解如何使用它们。

### 使用进程池

使用多个进程而不是线程会增加一些开销。大多数情况下，它会增加内存占用，因为每个进程都有自己独立的内存上下文。这意味着在多线程应用程序中允许无限数量的子进程可能比允许无限数量的线程更成问题。

> 如果操作系统支持写时复制 (COW) 语义的 fork() 系统调用，则启动新子进程的内存开销将大大减少。 COW 允许操作系统对相同的内存页面进行重复数据删除，并仅在其中一个进程尝试修改它们时才复制它们。例如，Linux 提供了具有 COW 语义的 fork() 系统调用，但 Windows 没有。此外，在长期运行的过程中，COW 的好处可能会减少。

在依赖于多进程的应用程序中控制资源使用的最佳模式是以类似于我们在使用线程池部分中描述的线程的方式构建进程池。

multiprocessing 模块的最佳之处在于它提供了一个随时可用的 Pool 类，可以为你处理管理多个进程工作者的所有复杂性。这个池实现大大减少了所需的样板数量和与双向通信相关的问题数量。你也不必手动使用 join() 方法，因为 Pool 可以用作上下文管理器（使用 with 语句）。这是我们之前的线程示例之一，重写后使用多进程模块中的 Pool 类：

```python
import time
from multiprocessing import Pool
import requests


SYMBOLS = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
BASES = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
POOL_SIZE = 4


def fetch_rates(base):
    response = requests.get(
        f"https://api.vatcomply.com/rates?base={base}"
    )
    response.raise_for_status()
    rates = response.json()["rates"]
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    return base, rates


def present_result(base, rates):
    rates_line = ", ".join(
        [f"{rates[symbol]:7.03} {symbol}" for symbol in SYMBOLS]
    )
    print(f"1 {base} = {rates_line}")


def main():
    with Pool(POOL_SIZE) as pool:
        results = pool.map(fetch_rates, BASES)
    for result in results:
        present_result(*result)


if __name__ == "__main__":
    started = time.time()
    main()
    elapsed = time.time() - started
    print()
    print("time elapsed: {:.2f}s".format(elapsed))
```

如你所见，处理工作池现在更简单了，因为我们不必维护自己的工作队列和 start()/join() 方法。如果出现问题，代码现在更易于维护和调试。实际上，唯一明确处理多进程的代码部分是 main() 函数：

```python
def main():
    with Pool(POOL_SIZE) as pool:
        results = pool.map(fetch_rates, BASES)
    for result in results:
        present_result(*result)
```

我们不再需要处理用于传递结果的显式队列，并且我们不必想知道当其中一个子进程引发异常时会发生什么。这是对我们必须从头开始构建工作池的情况的巨大改进。现在，我们甚至不需要关心通信通道，因为它们是在 Pool 类实现中隐式创建的。

这并不意味着多线程总是需要麻烦。下一节我们来看看如何使用 multiprocessing.dummy 作为多线程接口。

### 使用 multiprocessing.dummy 作为多线程接口

多进程模块的高级抽象（例如 Pool 类）比线程模块中提供的简单工具具有更大的优势。但这并不意味着多进程总是比多线程好。在很多用例中，线程可能是比进程更好的解决方案。对于需要低延迟和/或高资源效率的情况尤其如此。

尽管如此，这并不意味着当你想使用线程而不是进程时，你需要牺牲来自多进程模块的所有有用的抽象。有 multiprocessing.dummy 模块，它复制了多进程 API，但使用多线程而不是派生/生成新进程。

这使你可以减少代码中的样板数量，并具有更可插入的代码结构。例如，让我们再看看上一节中的 main() 函数。我们可以让用户控制使用哪个处理后端（进程或线程）。我们可以简单地通过替换 Pool 对象构造函数类来做到这一点，如下所示：

```python
from multiprocessing import Pool as ProcessPool 
from multiprocessing.dummy import Pool as ThreadPool 
 
 
def main(use_threads=False): 
    if use_threads: 
        pool_cls = ThreadPool 
    else: 
        pool_cls = ProcessPool 
 
    with pool_cls(POOL_SIZE) as pool: 
        results = pool.map(fetch_rates, BASES) 
 
    for result in results: 
        present_result(*result)
```

> 虚拟线程池也可以作为 ThreadPool 类从 multiprocessing.pool 模块中导入。它将具有相同的实现；实际的导入路径只是个人喜好的问题。

多进程模块的这一方面表明多进程和多线程有很多共同点。它们都依赖于操作系统来促进并发。它们也可以以类似的方式操作，并且经常使用类似的抽象来确保通信或内存安全。

一种完全不同的并发方法是异步编程，它不依赖任何操作系统功能来确保信息的并发处理。让我们在下一节中看看这种并发模型。

## 异步编程

异步编程在过去几年中获得了很大的关注。在 Python 3.5 中，我们终于获得了一些巩固异步执行概念的语法特性。但这并不意味着在 Python 3.5 之前异步编程是不可能的。许多库和框架是在很早之前提供的，其中大部分都起源于旧版本的 Python 2。甚至还有一个完整的 Python 替代实现，称为 Stackless Python，专注于这种单一的编程方法。

考虑 Python 中的异步编程的最简单方法是想象一些类似于线程的东西，但不涉及系统调度。这意味着异步程序可以并发处理信息，但执行上下文是在内部切换的，而不是由系统调度程序进行的。

但是，当然，我们不会使用线程来并发处理异步程序中的工作。许多异步编程解决方案使用不同种类的概念，并且根据实现的不同，它们的命名也不同。以下是一些用于描述此类并发程序实体的示例名称：

- Green threads 或 greenlet（greenlet、gevent 或 eventlet 项目）
- 协程（Python 3.5 原生异步编程）
- Tasklets（无堆栈 Python）

> 绿色线程这个名字来自于由 Sun Microsystems 公司的 Green Team 实现的 Java 语言的原始线程库。绿色线程在 Java 1.1 中被引入并在 Java 1.3 中被放弃

这些主要是相同的概念，但通常以略有不同的方式实现。

出于显而易见的原因，在本节中，我们将只关注 Python 原生支持的协程，从 3.5 版本开始。

### 协作多任务和异步 I/O

协作多任务处理是异步编程的核心。在这种计算机多任务处理方式中，启动上下文切换（到另一个进程或线程）不是操作系统的责任。相反，每个进程在空闲时都会主动释放控制权，以实现多个程序的同时执行。这就是为什么它被称为协作多任务处理。所有进程都需要协作才能顺利进行多任务处理。

操作系统中有时会采用这种多任务处理模型，但现在很难找到它作为系统级解决方案。这是因为存在设计不良的服务可能很容易破坏整个系统稳定性的风险。使用由操作系统直接管理的上下文切换的线程和进程调度现在是操作系统级别的主要并发方法。但是协作多任务在应用程序层面仍然是一个很好的并发工具。

在应用程序级别进行协作多任务时，我们不处理需要释放控制权的线程或进程，因为所有执行都包含在单个进程和线程中。相反，我们有多个任务（协程、小任务或绿色线程）将控制权释放给处理任务协调的单个函数。这个函数通常是某种事件循环。

> 为了避免以后混淆（由于 Python 术语），从现在开始，我们将此类并发任务称为协程。

协作式多任务处理中最重要的问题是何时释放控制权。在大多数异步应用程序中，控制权被释放给 I/O 操作的调度程序或事件循环。程序是从文件系统读取数据还是通过套接字进行通信都没有关系，因为当进程空闲时，I/O 操作总是会导致一些等待时间。等待时间取决于外部资源，因此这是一个释放控制权的好机会，以便其他协程可以完成它们的工作，直到它们也需要等待。

这使得这种方法在行为上与 Python 中的多线程实现方式有些相似。我们知道 GIL 会序列化 Python 线程，但它也会在每次 I/O 操作时释放。主要区别在于 Python 中的线程被实现为系统级线程，以便操作系统可以抢占当前运行的线程并在任何时间点将控制权交给另一个线程。在异步编程中，任务永远不会被主事件循环抢占，而是必须显式地返回控制权。这就是为什么这种多任务处理方式也被称为非抢占式多任务处理。这减少了上下文切换的时间损失，并更好地与 CPython 的 GIL 实现配合使用。

当然，每个 Python 应用程序都运行在有其他进程争用资源的操作系统上。这意味着操作系统始终有权抢占整个进程并将控制权交给另一个进程。但是当我们的异步应用程序运行时，它会从系统调度器介入时暂停的同一个地方继续运行。这就是为什么协程仍然被认为是非抢占式的。

在下一节中，我们将看看 async 和 await 关键字，它们是 Python 中协作多任务处理的支柱。

### Python async 和 await 关键字

async 和 await 关键字是 Python 异步编程的主要构建块。

async 关键字在 def 语句之前使用时，定义了一个新的协程。协程函数的执行可以在严格定义的情况下暂停和恢复。它的语法和行为与生成器非常相似。事实上，只要你想实现协程，就需要在旧版本的 Python 中使用生成器。下面是一个使用 async 关键字的函数声明示例：

```python
async def async_hello(): 
    print("hello, world!")
```

用 async 关键字定义的函数是特殊的。当被调用时，它们不会执行里面的代码，而是返回一个协程对象。考虑以下来自交互式 Python 会话的示例：

```python
>>> async def async_hello():
...     print("hello, world!")
... 
>>> async_hello()
<coroutine object async_hello at 0x1014129e8>
```

协程对象不会做任何事情，直到它的执行在事件循环中被调度。 asyncio 模块可用于提供基本的事件循环实现以及许多其他异步实用程序。以下示例尝试在交互式 Python 会话中手动安排协程执行：

```python
>>> import asyncio
>>> async def async_hello():
...     print("hello, world!")
... 
>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(async_hello())
hello, world!
>>> loop.close()
```

显然，由于我们只创建了一个简单的协程，因此我们的程序中不涉及并发。为了查看实际并发的内容，我们需要创建更多将由事件循环执行的任务。

可以通过调用 loop.create_task() 方法或向 asyncio.wait() 函数提供“awaitable”对象来将新任务添加到循环中。如果你有多个任务或协程要等待，你可以使用 asyncio.gather() 将它们聚合成一个对象。我们将使用后一种方法并尝试异步打印使用 range() 函数生成的数字序列，如下所示：

```python
import asyncio 
import random
 
async def print_number(number):
    await asyncio.sleep(random.random())
    print(number) 
 
 
if __name__ == "__main__": 
    loop = asyncio.get_event_loop() 
 
    loop.run_until_complete( 
        asyncio.gather(*[ 
            print_number(number) 
            for number in range(10) 
        ]) 
    ) 
    loop.close()
```

让我们将脚本保存在 async_print.py 文件中，看看它是如何工作的：

```python
python async_print.py
```

你将看到的输出可能如下所示：

```python
0
7
8
3
9
4
1
5
2
6
```

asyncio.gather() 函数接受多个协程对象并立即返回。它接受可变数量的位置参数。这就是我们使用参数解包语法（* 运算符）将协程列表解包为参数的原因。顾名思义，asyncio.gather() 用于收集多个协程以并发执行它们。结果是一个对象，表示运行所有提供的协程的未来结果（所谓的未来）。 loop.run_until_complete() 方法运行事件循环，直到给定的未来完成。

我们使用 asyncio.sleep(random.random()) 来强调协程的异步操作。多亏了这一点，协程可以相互交织。

我们无法用普通的 time.sleep() 函数实现相同的结果（即协程的交织）。协程在释放执行控制权时可以开始交织。这是通过 await 关键字完成的。它暂停正在等待另一个协程或未来结果的协程的执行。

每当一个函数等待时，它就会将执行的控制权释放给事件循环。为了更好地理解这是如何工作的，我们需要查看一个更复杂的代码示例。

假设我们要创建两个协程，它们都将在循环中执行相同的简单任务：

- 等待随机数秒
- 打印一些作为参数提供的文本和睡眠时间

让我们从以下不使用 await 关键字的简单实现开始：

```python
import time
import random


async def waiter(name): 
    for _ in range(4): 
        time_to_sleep = random.randint(1, 3) / 4 
        time.sleep(time_to_sleep) 
        print(f"{name} waited { time_to_sleep } seconds")
```

我们可以使用 asyncio.gather() 调度多个 waiter() 协程的执行，就像我们在 async_print.py 脚本中所做的那样：

```python
import time
import random


async def waiter(name): 
    for _ in range(4): 
        time_to_sleep = random.randint(1, 3) / 4 
        time.sleep(time_to_sleep) 
        print(f"{name} waited { time_to_sleep } seconds")
```

让我们将代码保存在 waiters.py 文件中，看看这两个 waiter() 协程如何在事件循环中执行：

```python
$ time python3 waiters.py
```

请注意，我们已使用时间实用程序来测量总执行时间。前面的执行可以给出以下输出：

```python
$ time python waiters.py
first waited 0.25 seconds
first waited 0.75 seconds
first waited 0.5 seconds
first waited 0.25 seconds
second waited 0.75 seconds
second waited 0.5 seconds
second waited 0.5 seconds
second waited 0.75 seconds
real    0m4.337s
user    0m0.050s
sys     0m0.014s
```

正如我们所见，两个协程都完成了它们的执行，但不是以异步方式。原因是它们都使用了 time.sleep() 函数，该函数会阻塞但不会将控制权释放给事件循环。这在多线程设置中效果更好，但我们现在不想使用线程。那么，我们该如何解决这个问题呢？

答案是使用 asyncio.sleep()，它是 time.sleep() 的异步版本，并使用 await 关键字等待其结果。让我们看看以下改进版的 waiter() 协程，它使用了 await asyncio.sleep() 语句：

```python
async def waiter(name):
    for _ in range(4):
        time_to_sleep = random.randint(1, 3) / 4
        await asyncio.sleep(time_to_sleep)
        print(f"{name} waited {time_to_sleep} seconds")
```

如果我们在 waiters_await.py 文件中保存此脚本的修改版本并在 shell 中执行它，我们将有望看到两个函数的输出如何相互交织：

```python
$ time python waiters_await.py
```

你将看到的输出应类似于以下内容：

```python
first waited 0.5 seconds
second waited 0.75 seconds
second waited 0.25 seconds
first waited 0.75 seconds
second waited 0.75 seconds
first waited 0.75 seconds
second waited 0.75 seconds
first waited 0.5 seconds
real    0m2.589s
user    0m0.053s
sys     0m0.016s
```

这种简单改进的额外优势是代码运行速度更快。整体执行时间小于所有休眠时间的总和，因为协程协同释放控制权。

下一节我们来看一个更实际的异步编程示例。

### 异步编程的一个实际例子

正如我们在本章中多次提到的，异步编程是处理 I/O 绑定操作的好工具。因此，是时候构建比简单打印序列或异步等待更实用的东西了。

为了一致性，我们将尝试在多线程和多进程的帮助下处理我们之前解决的相同问题。因此，我们将尝试通过网络连接从外部资源异步获取有关当前货币汇率的一些信息。如果我们可以使用与前几节相同的请求库，那就太好了。不幸的是，我们不能这样做。或者更准确地说，我们无法有效地做到这一点。

不幸的是，requests 库不支持带有 async 和 await 关键字的异步 I/O。还有一些其他项目旨在为请求项目提供一些并发性，但它们要么依赖 Gevent（如 grequests，可在 https://github.com/kennethreitz/grequests 获得）或线程/进程池执行（如requests-futures，可在 https://github.com/ross/requests-futures 获得）。这些都不能解决我们的问题。

了解在我们之前的示例中非常容易使用的库的局限性，我们需要构建一些可以填补空白的东西。外汇汇率 API 使用起来非常简单，所以我们只需要使用原生异步 HTTP 库来完成这项工作。 3.9 版本的 Python 标准库仍然缺乏任何可以使异步 HTTP 请求像调用 urllib.urlopen() 一样简单的库。我们绝对不想从头开始构建整个协议支持，所以我们将使用 aiohttp 包的一些帮助，它在 PyPI 上可用。这是一个非常有前途的库，它为异步 HTTP 添加了客户端和服务器实现。这是一个建立在 aiohttp 之上的小模块，它创建了一个 get_rates() 辅助函数，用于向外汇汇率 API 服务发出请求：

```python
import aiohttp
async def get_rates(session: aiohttp.ClientSession, base: str):
    async with session.get(
        f"https://api.vatcomply.com/rates?base={base}"
    ) as response:
        rates = (await response.json())['rates']
        rates[base] = 1.
        return base, rates
```

我们将该模块保存在 asyncrates.py 文件中，以便稍后我们能够将其作为 asyncrates 模块导入。

现在，我们准备重写讨论多线程和多进程时使用的示例。以前，我们将整个操作拆分为以下两个单独的步骤：

- 使用 asyncrates.get_rates() 函数并行执行对外部服务的所有请求
- 使用 present_result() 函数在循环中显示所有结果

我们程序的核心将是一个简单的 main() 函数，它从多个 get_rates() 协程收集结果并将它们传递给 present_result() 函数：

```python
async def main():
    async with aiohttp.ClientSession() as session:
        for result in await asyncio.gather(*[
            get_rates(session, base)
            for base in BASES
        ]):
            present_result(*result)
```

完整的代码，连同导入和事件循环初始化，将如下所示：

```python
import asyncio
import time
import aiohttp
from asyncrates import get_rates


SYMBOLS = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')
BASES = ('USD', 'EUR', 'PLN', 'NOK', 'CZK')


def present_result(base, rates):
    rates_line = ", ".join(
        [f"{rates[symbol]:7.03} {symbol}" for symbol in SYMBOLS]
    )
    print(f"1 {base} = {rates_line}")


async def main():
    async with aiohttp.ClientSession() as session:
        for result in await asyncio.gather(*[
            get_rates(session, base)
            for base in BASES
        ]):
            present_result(*result)


if __name__ == "__main__":
    started = time.time()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
    elapsed = time.time() - started
    print()
```

运行此程序的输出将类似于依赖于多线程和多进程的版本的输出：

```python
$ python async_aiohttp.py
1 USD =     1.0 USD,   0.835 EUR,    3.81 PLN,    8.39 NOK,    21.7 CZK
1 EUR =     1.2 USD,     1.0 EUR,    4.56 PLN,    10.0 NOK,    25.9 CZK
1 PLN =   0.263 USD,    0.22 EUR,     1.0 PLN,     2.2 NOK,    5.69 CZK
1 NOK =   0.119 USD,  0.0996 EUR,   0.454 PLN,     1.0 NOK,    2.58 CZK
1 CZK =  0.0461 USD,  0.0385 EUR,   0.176 PLN,   0.387 NOK,     1.0 CZK
time elapsed: 0.33s
```

与多线程和多进程相比，使用 asyncio 的优势在于我们不必处理进程池和内存安全来实现并发网络通信。缺点是我们不能使用像 requests 包这样的流行同步通信库。我们改用 aiohttp，这对于简单的 API 来说相当容易。但有时，你需要一个非异步且不易移植的专用客户端库。我们将在下一节中介绍这种情况。

### 使用futures将非异步代码与异步集成

异步编程很棒，特别是对于对构建可扩展应用程序感兴趣的后端开发人员。在实践中，它是构建高并发服务器的最重要工具之一。

但现实是痛苦的。许多进程 I/O 绑定问题的流行包并不打算与异步代码一起使用。主要原因如下：

- 高级 Python 3 特性的采用率低（尤其是异步编程）
- Python初学者对各种并发概念的理解偏低

这意味着，现有同步多线程应用程序和包的迁移通常是不可能的（由于架构限制）或成本太高。许多项目可以从合并多任务的异步风格中受益匪浅，但只有少数项目最终会这样做。

这意味着现在，你在尝试从头开始构建异步应用程序时会遇到很多困难。在大多数情况下，这将类似于异步编程的一个实际示例部分中提到的请求库的问题——不兼容的接口和 I/O 操作的同步阻塞。

当然，当你遇到这种不兼容的情况时，你有时可以退出 await 并同步获取所需的资源。但这将阻止所有其他协程在你等待结果时执行其代码。它在技术上有效，但也破坏了异步编程的所有收益。所以，归根结底，将异步 I/O 与同步 I/O 结合起来并不是一种选择。这是一种全有或全无的游戏。

另一个问题是长时间运行的 CPU 密集型操作。在执行 I/O 操作时，从协程中释放控制不是问题。从套接字写入/读取时，你最终会等待，因此使用 await 是你能做的最好的事情。但是当你需要实际计算某些东西并且你知道这需要一段时间时，你应该怎么做？当然，你可以将问题分成几部分，并在每次将工作向前推进一点时使用 asyncio.wait(0) 释放控制权。但是你很快就会发现这不是一个好的模式。这样的事情会让代码变得一团糟，也不能保证好的结果。时间切片应该是解释器或操作系统的责任。

那么，如果你有一些代码会进行长时间的同步 I/O 操作而你不能或不愿意重写，你应该怎么做？或者，当你必须在一个主要设计为异步 I/O 的应用程序中进行一些繁重的 CPU 密集型操作时，你应该怎么做？嗯......你需要使用一种解决方法。通过变通方法，我的意思是多线程或多进程。

这听起来可能并不明显，但有时最好的解决方案可能是我们试图逃避的解决方案。使用多进程在 Python 中并行处理 CPU 密集型任务总是更好。如果你正确设置并小心处理，多线程也可以像异步和等待一样处理 I/O 操作（快速且没有大量资源开销）。

因此，当某些东西根本不适合你的异步应用程序时，请使用一段代码将其推迟到单独的线程或进程。你可以假装这是一个协程，然后使用 await 将控制权释放给事件循环。当结果准备好时，你最终将对其进行处理。对我们来说幸运的是，Python 标准库提供了 concurrent.futures 模块，该模块也与 asyncio 模块集成。这两个模块一起允许你安排阻塞函数在线程或附加进程中执行，就好像它们是异步非阻塞协程一样。

让我们在下一节中仔细看看 executors 和 futures。

#### executors 和 futures

在我们看到如何将线程或进程注入异步事件循环之前，我们将仔细研究 concurrent.futures 模块，它稍后将成为我们所谓的解决方法的主要成分。 concurrent.futures 模块中最重要的类是 Executor 和 Future。

Executor 代表可以并行处理工作项的资源池。这可能看起来与多进程模块中的类（Pool 和 dummy.Pool）非常相似，但它具有完全不同的接口和语义。 Executor 类是一个不用于实例化的基类，具有以下两个具体实现：

- ThreadPoolExecutor：这是一个代表线程池的
- ProcessPoolExecutor：这是一个代表进程池的

每个 executor 提供以下三种方法：

- submit(func, *args, **kwargs)：这会安排 func 函数在资源池中执行，并返回表示可调用对象执行的 Future 对象
- map(func, *iterables, timeout=None, chunksize=1): 以类似于 multiprocessing.Pool.map() 方法的方式在一个可迭代对象上执行 func 函数
- shutdown(wait=True)：这会关闭执行器并释放其所有资源

最有趣的方法是 submit() 因为它返回 Future 对象。它表示可调用对象的异步执行，并且仅间接表示其结果。为了获得提交的callable的实际返回值，需要调用Future.result()方法。如果可调用对象已经完成，则 result() 方法不会阻塞，只会返回函数输出。如果它不是真的，它会阻塞直到结果准备好。把它当作一个结果的承诺（实际上，它与 JavaScript 中的承诺是相同的概念）。你不需要在收到后立即解包（使用 result() 方法），但如果你尝试这样做，则保证最终会返回一些内容。

让我们考虑在交互式 Python 会话中与 ThreadPoolExecutor 的以下交互：

```python
>>> def loudly_return():
...     print("processing")
...     return 42
...     
>>> from concurrent.futures import ThreadPoolExecutor
>>> with ThreadPoolExecutor(1) as executor:
...     future = executor.submit(loudly_return)
...     
processing
>>> future
<Future at 0x33cbf98 state=finished returned int>
>>> future.result()
42
```

如你所见，loudly_return() 在提交给执行程序后立即打印处理字符串。这意味着执行甚至在我们决定使用 future.result() 方法解压其值之前就开始了。

在下一节中，我们将看到如何在事件循环中使用执行程序。

#### 在事件循环中使用Executor

Executor.submit() 方法返回的 Future 类实例在概念上非常接近异步编程中使用的协程。这就是为什么我们可以使用执行器在协作多任务和多进程或多线程之间进行混合。

此解决方法的核心是事件循环类的 run_in_executor(executor, func, *args) 方法。它允许你在由 executor 参数表示的进程或线程池中调度 func 函数的执行。该方法最重要的一点是它返回一个新的可等待对象（可以使用 await 语句等待的对象）。因此，多亏了这一点，你可以像执行协程一样执行不是协程的阻塞函数。最重要的是，它不会阻止事件循环处理其他协程，无论需要多长时间才能完成。它只会停止正在等待此类调用结果的函数，但整个事件循环仍将继续旋转。

一个有用的事实是，你甚至不需要创建执行程序实例。如果你将 None 作为 executor 参数传递，则 ThreadPoolExecutor 类将与默认线程数一起使用（对于 Python 3.9，它是处理器数乘以 5）。

因此，让我们假设我们不想重写导致我们头痛的面向 API 的代码中有问题的部分。我们可以使用 loop.run_in_executor() 调用轻松地将阻塞调用推迟到单独的线程，同时仍将 fetch_rates() 函数保留为可等待的协程，如下所示：

```python
async def fetch_rates(base):
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(
        None, requests.get,
        f"https://api.vatcomply.com/rates?base={base}"
    )
    response.raise_for_status()
    rates = response.json()["rates"]
    # note: same currency exchanges to itself 1:1
    rates[base] = 1.
    return base, rates
```

这样的解决方案不如有一个完全异步的库来完成这项工作，但半条面包总比没有好。

异步编程是构建高性能并发应用程序的绝佳工具，这些应用程序必须通过网络与其他服务进行大量通信。你可以轻松地做到这一点，而不会出现多线程和（在某种程度上）多进程通常带来的所有内存安全问题。缺乏无意识的上下文切换也减少了必要的锁定原语的数量，因为很容易预测协程何时将控制权返回给事件循环。

不幸的是，这是以必须使用专用异步库为代价的。同步和线程应用程序通常具有更好的客户端和通信库覆盖范围，以便与流行的服务进行交互。 Executors 和 future 允许你填补这一空白，但不如本地异步解决方案最佳。

## 概括

这是一段漫长的旅程，但我们成功地克服了 Python 程序员可用的大多数并发编程的常见方法。

在解释了并发到底是什么之后，我们开始行动并借助多线程剖析了一个典型的并发问题。在确定我们代码的基本缺陷并修复它们之后，我们转向多进程以查看它在我们的情况下如何工作。我们发现带有 multiprocessing 模块的多个进程比带有 threading 模块的普通线程更容易使用。但在那之后，我们意识到我们也可以对线程使用相同的 API，这要归功于 multiprocessing.dummy 模块。因此，多进程和多线程之间的决定现在只是哪个解决方案更适合问题的问题，而不是哪个解决方案具有更好的接口。

说到问题匹配，我们最终尝试了异步编程，这应该是 I/O 密集型应用程序的最佳解决方案，但才意识到我们不能完全忘记线程和进程。所以，我们做了一个圆圈！回到我们开始的地方。

这使我们得出本章的最终结论。没有银弹。有一些你可能更喜欢的方法。有一些方法可能更适合一组给定的问题，但你需要全部了解它们才能成功。在实际场景中，你可能会发现自己在单个应用程序中使用了所有并发工具和样式，这并不少见。

在下一章中，我们将看一个与并发有些相关的主题：事件驱动编程。在那一章中，我们将专注于构成分布式异步和高并发系统主干的各种通信模式。
